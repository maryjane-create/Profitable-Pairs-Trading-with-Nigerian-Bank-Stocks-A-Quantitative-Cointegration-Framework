# -*- coding: utf-8 -*-
"""MJ-NGX_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmC06X-n3wJKtHIdQEfjskn5de9IEoAP
"""

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
from google.colab import files

# Step 1: Upload CSV Files
def upload_csv_files():
    """
    Upload CSV files containing historical prices for the given tickers.
    """
    uploaded = files.upload()
    data_frames = {}
    for file_name, file_content in uploaded.items():
        # Assume the first column contains the datetime data if 'datetime' is not found
        try:
            df = pd.read_csv(file_name, index_col='datetime', parse_dates=True)
        except ValueError:
            df = pd.read_csv(file_name, index_col=0, parse_dates=True)
            # If 'datetime' is not found, use the first column (index 0) as index
            df.index.name = 'datetime'  # Rename the index to 'datetime'
            print(f"Warning: 'datetime' column not found in {file_name}. Using the first column as index instead.")
        data_frames[file_name] = df
    return data_frames

# Step 2: Calculate Correlation Tables (using Close prices)
def calculate_correlation(data):
    """
    Calculate the correlation matrix for the given data using the 'Close' column.
    """
    # Select only the 'Close' column for correlation calculation
    close_prices = data[['close']]  # Assuming your CSV has a 'Close' column
    return close_prices.corr()

# Step 3: Perform Cointegration Tests (using Close prices)
def perform_cointegration_tests(data):
    """
    Perform cointegration tests for all pairs of assets using the 'Close' column.
    """
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                # Select the 'Close' column for both assets
                score, pvalue, _ = coint(data[asset1]['Close'], data[asset2]['Close'])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
                print(f"Make sure your DataFrame has a 'Close' column for asset1 and asset2.")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

# Upload CSV files
price_data = upload_csv_files()

# Print valid tickers
print("\nValid Tickers Found:")
for file_name, data in price_data.items():
    print(f"\n{file_name}:")
    print(data.columns.tolist())

# Generate correlation tables for each uploaded file
correlation_tables = {}
for file_name, data in price_data.items():
    correlation_tables[file_name] = calculate_correlation(data)

# Print correlation tables to the console
print("\n=== Correlation Tables ===")
for file_name, corr_table in correlation_tables.items():
    print(f"\n{file_name} Correlation Table:")
    print(corr_table)

# Perform cointegration tests for each uploaded file
cointegration_results = {}
for file_name, data in price_data.items():
    cointegration_results[file_name] = perform_cointegration_tests(data)

# Print cointegration results to the console
print("\n=== Cointegration Results ===")
for file_name, results in cointegration_results.items():
    print(f"\n{file_name} Cointegration Results (p-value < 0.05):")
    significant_pairs = results[results['P-Value'] < 0.05]
    if not significant_pairs.empty:
        print(significant_pairs)
    else:
        print("No significant cointegrated pairs found.")

!pip install investpy pandas numpy statsmodels

import pandas as pd
import numpy as np
import yfinance as yf
from statsmodels.tsa.stattools import coint

# Step 1: Fetch Historical Prices
def fetch_prices(tickers, start_date, end_date):
    """
    Fetch historical adjusted close prices for the given tickers.
    Filter out tickers with missing data.
    """
    try:
        data = yf.download(tickers, start=start_date, end=end_date)['Close']
        # Drop tickers with all NaN values (missing data)
        data = data.dropna(axis=1, how='all')
        return data
    except Exception as e:
        print(f"Error fetching data: {e}")
        return None

# Define the list of tickers
tickers = ['GTCO', 'ZENITHBANK', 'UBA', 'DANGCEM', 'FIDELITYBANK', 'DJI', 'NQX', '^SOX',
           'GOOGL', 'META', 'NVDA', 'CSCO', 'IGM', '^IXIC', 'QQQ', '^GSPC',
           'AAPL', 'MSFT', 'AMZN']

# Define timeframes
timeframes = {
    '2-year': ('2021-01-01', '2023-01-01'),
    '5-year': ('2018-01-01', '2023-01-01'),
    '8-year': ('2015-01-01', '2023-01-01'),
    '10-year': ('2013-01-01', '2023-01-01')
}

# Fetch data for all timeframes
price_data = {}
valid_tickers = set()  # Track valid tickers across all timeframes
for name, (start, end) in timeframes.items():
    data = fetch_prices(tickers, start, end)
    if data is not None:
        price_data[name] = data
        valid_tickers.update(data.columns)

# Print valid tickers
print("\nValid Tickers Found:")
print(valid_tickers)

# Step 2: Calculate Correlation Tables
def calculate_correlation(data):
    """
    Calculate the correlation matrix for the given data.
    """
    return data.corr()

# Generate correlation tables for each timeframe
correlation_tables = {}
for name, data in price_data.items():
    correlation_tables[name] = calculate_correlation(data)

# Print correlation tables to the console
print("\n=== Correlation Tables ===")
for name, corr_table in correlation_tables.items():
    print(f"\n{name} Correlation Table:")
    print(corr_table)

# Step 3: Perform Cointegration Tests
def perform_cointegration_tests(data):
    """
    Perform cointegration tests for all pairs of assets.
    """
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

# Perform cointegration tests for each timeframe
cointegration_results = {}
for name, data in price_data.items():
    cointegration_results[name] = perform_cointegration_tests(data)

# Print cointegration results to the console
print("\n=== Cointegration Results ===")
for name, results in cointegration_results.items():
    print(f"\n{name} Cointegration Results (p-value < 0.05):")
    significant_pairs = results[results['P-Value'] < 0.05]
    if not significant_pairs.empty:
        print(significant_pairs)
    else:
        print("No significant cointegrated pairs found.")

# import pandas as pd
# import numpy as np
# from statsmodels.tsa.stattools import coint
# import investpy

# # Step 1: Fetch Historical Prices
# def fetch_prices(tickers, country, start_date, end_date):
#     """
#     Fetch historical adjusted close prices for the given tickers using investpy.
#     Filter out tickers with missing data.
#     """
#     data = pd.DataFrame()
#     for ticker in tickers:
#         try:
#             # Fetch historical data for each ticker
#             stock_data = investpy.get_stock_historical_data(
#                 stock=ticker,
#                 country=country,
#                 from_date=start_date,
#                 to_date=end_date
#             )['Close']
#             stock_data.name = ticker  # Rename column to ticker name
#             data = pd.concat([data, stock_data], axis=1)
#         except Exception as e:
#             print(f"Error fetching data for {ticker}: {e}")
#     # Drop tickers with all NaN values (missing data)
#     data = data.dropna(axis=1, how='all')
#     return data

# # Define the list of Nigerian stock tickers
# tickers = [
#     'GTCO', 'ZENITHBANK', 'UBA', 'ACCESS', 'STANBIC',
#     'FBNH', 'DANGSUGAR', 'DANGCEM', 'MTNN', 'SEPLAT',
#     'NESTLE', 'NB', 'PZ', 'FLOURMILL', 'GUINNESS',
#     'WAPCO', 'BUACEMENT', 'FIDELITYBK', 'TRANSCORP', 'UCAP'
# ]

# # Define timeframes
# timeframes = {
#     '2-year': ('01/01/2021', '01/01/2023'),
#     '5-year': ('01/01/2018', '01/01/2023'),
#     '8-year': ('01/01/2015', '01/01/2023'),
#     '10-year': ('01/01/2013', '01/01/2023')
# }

# # Fetch data for all timeframes
# price_data = {}
# valid_tickers = set()  # Track valid tickers across all timeframes
# for name, (start, end) in timeframes.items():
#     data = fetch_prices(tickers, country="Nigeria", start_date=start, end_date=end)
#     if not data.empty:
#         price_data[name] = data
#         valid_tickers.update(data.columns)

# # Print valid tickers
# print("\nValid Tickers Found:")
# print(valid_tickers)

# # Step 2: Calculate Correlation Tables
# def calculate_correlation(data):
#     """
#     Calculate the correlation matrix for the given data.
#     """
#     return data.corr()

# # Generate correlation tables for each timeframe
# correlation_tables = {}
# for name, data in price_data.items():
#     correlation_tables[name] = calculate_correlation(data)

# # Print correlation tables to the console
# print("\n=== Correlation Tables ===")
# for name, corr_table in correlation_tables.items():
#     print(f"\n{name} Correlation Table:")
#     print(corr_table)

# # Step 3: Perform Cointegration Tests
# def perform_cointegration_tests(data):
#     """
#     Perform cointegration tests for all pairs of assets.
#     """
#     n = len(data.columns)
#     results = []
#     for i in range(n):
#         for j in range(i + 1, n):
#             asset1 = data.columns[i]
#             asset2 = data.columns[j]
#             try:
#                 score, pvalue, _ = coint(data[asset1], data[asset2])
#                 results.append((asset1, asset2, score, pvalue))
#             except Exception as e:
#                 print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
#     return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

# # Perform cointegration tests for each timeframe
# cointegration_results = {}
# for name, data in price_data.items():
#     cointegration_results[name] = perform_cointegration_tests(data)

# # Print cointegration results to the console
# print("\n=== Cointegration Results ===")
# for name, results in cointegration_results.items():
#     print(f"\n{name} Cointegration Results (p-value < 0.05):")
#     significant_pairs = results[results['P-Value'] < 0.05]
#     if not significant_pairs.empty:
#         print(significant_pairs)
#     else:
#         print("No significant cointegrated pairs found.")

import investpy
import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint

# Step 1: Fetch Historical Prices using investpy
def fetch_prices_investpy(stock, country, from_date, to_date):
    """
    Fetch historical adjusted close prices for the given stock from investpy.
    """
    try:
        df = investpy.get_stock_historical_data(
            stock=stock,
            country=country,
            from_date=from_date,
            to_date=to_date
        )
        return df[['Close']].rename(columns={'Close': stock.upper()})
    except Exception as e:
        print(f"Error fetching {stock}: {e}")
        return None

# Step 2: Define your Nigerian stocks (as recognized by investpy)
nigerian_stocks = [
    'GTBank',         # GTCO
    'Zenith Bank',    # Zenith Bank
    'UBA',            # United Bank for Africa
    'Access Bank',    # Access Holdings
    'Stanbic IBTC'    # Stanbic IBTC Holdings
]

# Step 3: Download data and merge
def get_all_prices(stocks, from_date, to_date):
    all_data = []
    for stock in stocks:
        df = fetch_prices_investpy(stock, 'nigeria', from_date, to_date)
        if df is not None:
            all_data.append(df)
    if all_data:
        combined = pd.concat(all_data, axis=1).dropna()
        return combined
    else:
        return pd.DataFrame()

# Define date range
from_date = '01/01/2021'
to_date = '01/01/2023'

# Fetch data
price_data = get_all_prices(nigerian_stocks, from_date, to_date)

# Step 4: Calculate Correlation Matrix
print("\n=== Correlation Matrix ===")
correlation_matrix = price_data.corr()
print(correlation_matrix)

# Step 5: Perform Cointegration Tests
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error testing {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

cointegration_results = perform_cointegration_tests(price_data)

# Filter for significant p-values
significant_pairs = cointegration_results[cointegration_results['P-Value'] < 0.05]

print("\n=== Significant Cointegrated Pairs (p < 0.05) ===")
print(significant_pairs if not significant_pairs.empty else "None found.")

import requests
import pandas as pd

API_KEY = '2bf4a5b5ab9a4a9e8c224f37457f7387'  # Replace with your Twelve Data API key
symbol = 'GTCO.XNG'  # GTCO on Nigerian Exchange (or .NGN or .XNG)

def fetch_nigerian_data(symbol, interval='1day', outputsize=5000):
    url = f"https://api.twelvedata.com/time_series?symbol={symbol}&interval={interval}&outputsize={outputsize}&apikey={API_KEY}"
    response = requests.get(url)
    data = response.json()

    if 'values' not in data:
        print("Error:", data.get('message', 'No data returned.'))
        return pd.DataFrame()

    df = pd.DataFrame(data['values'])
    df['datetime'] = pd.to_datetime(df['datetime'])
    df.set_index('datetime', inplace=True)
    df = df.sort_index()
    return df

# Example: Fetch GTCO data
df_gtco = fetch_nigerian_data('GTCO.XNG')  # or 'ZENITHBANK.XNG', etc.
print(df_gtco.head())

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
import os

# Step 1: Read 20 user-specified files containing 'Date' and 'Close' columns

# Example file paths – REPLACE these with your actual file paths
file_paths = {
    'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
    'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
    'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
    'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
    'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
    'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
    'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
    'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
    'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
    'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
    'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
    'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
    # 'UNIONBANK': '/path/to/XOM.xlsx',
    'FIDELITYBANK': '/content/NSENG_FIDELITYBK, 1D_11cce.csv',
    'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
    'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
    'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
    'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
    'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
    'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
    # 'GSK': '/path/to/T.xlsx',
    'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
    'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
    'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
    'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
    'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
    'UPDC': '/content/NSENG_UPDC, 1D_58c00.csv',
    'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
    'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
    'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
    'PZ':'/content/NSENG_PZ, 1D_a1fb5.csv',
    'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
    'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv',
    'AIICO':'/content/NSENG_AIICO, 1D_71aa9 (1).csv',
    'AIRTEL':'/content/NSENG_AIRTELAFRI, 1D_1e3ed (1).csv',
    'ARADEL':'/content/NSENG_ARADEL, 1D_1b7ee (1).csv',
    'BUAFOOD':'/content/NSENG_BUAFOODS, 1D_4869e (1).csv',
    'CORNERSTONE':'/content/NSENG_CORNERST, 1D_745a2 (1).csv',
    'DANGSUGAR':'/content/NSENG_DANGSUGAR, 1D_601ba (1).csv',
    'ETI':'/content/NSENG_ETI, 1D_26f95.csv',
    'FBNH':'/content/NSENG_FBNH, 1D_15c8b (1).csv',
    'GEREGU':'/content/NSENG_GEREGU, 1D_ee307 (1).csv',
    'INTBREW':'/content/NSENG_INTBREW, 1D_84d73 (1).csv',
    'JAIZBANK':'/content/NSENG_JAIZBANK, 1D_bc6e8.csv',
    'MANSARD':'/content/NSENG_MANSARD, 1D_52cd5 (2).csv',
    'NB':'/content/NSENG_NB, 1D_45d75.csv',
    'OKOMUOIL':'/content/NSENG_OKOMUOIL, 1D_87901.csv',
    'OMATEK':'/content/NSENG_OMATEK, 1D_b5529 (1).csv',
    'PRESCO':'/content/NSENG_PRESCO, 1D_15e76 (1).csv',
    'STERLINGBANK':'/content/NSENG_STERLINGNG, 1D_ec3d0 (1).csv',
    'TIP':'/content/NSENG_TIP, 1D_b2cfb (1) (1).csv',
    'TRANSCORPHOTEL':'/content/NSENG_TRANSCOHOT, 1D_ed216.csv',
    'TRANSPOWER':'/content/NSENG_TRANSPOWER, 1D_081ff (1).csv',
    'UACN':'/content/NSENG_UACN, 1D_6163f (1).csv',
    'UCAP':'/content/NSENG_UCAP, 1D_181d9.csv',
    'UPDC':'/content/NSENG_UPDC, 1D_58c00 (1).csv',
    'WAPCO':'/content/NSENG_WAPCO, 1D_10d10 (1).csv',
    'WAPIC':'/content/NSENG_WAPIC, 1D_b83aa (2).csv',



}

# Step 2: Load and merge all data into a single DataFrame
def load_close_prices(file_paths):
    all_data = []
    for ticker, path in file_paths.items():
        try:
            # Check if the file exists before attempting to read it
            if os.path.exists(path):
                df = pd.read_excel(path) if path.endswith(".xlsx") else pd.read_csv(path)
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
                df = df[['close']].rename(columns={'close': ticker})
                all_data.append(df)
            else:
                print(f"File not found: {path} for ticker {ticker}") # Print a warning if the file is not found

        except Exception as e:
            print(f"Error reading {ticker}: {e}")

    # Check if all_data is empty before concatenating
    if all_data:
        merged = pd.concat(all_data, axis=1)
        merged.sort_index(inplace=True)
        return merged.dropna(axis=1, how='all')  # Drop tickers with all missing values
    else:
        print("No data loaded from any files. Check file paths and formats.")
        return pd.DataFrame()  # Return an empty DataFrame if no data is loaded

price_data = load_close_prices(file_paths)

# Print loaded tickers
print("\nLoaded Tickers:")
print(list(price_data.columns))

# Step 3: Calculate Correlation Table
def calculate_correlation(data):
    return data.corr()

correlation_table = calculate_correlation(price_data)
print("\n=== Correlation Table ===")
print(correlation_table)
correlation_table.to_csv("correlation_table.csv")

# Step 4: Cointegration Tests
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

cointegration_results = perform_cointegration_tests(price_data)

print("\n=== Cointegration Results (p-value < 0.05) ===")
significant = cointegration_results[cointegration_results['P-Value'] < 0.05]
if not significant.empty:
    print(significant)
    significant.to_csv("cointegration_results.csv", index=False)
else:
    print("No significant cointegrated pairs found.")

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
import os


# Step 1: Read 20 user-specified files containing 'Date' and 'Close' columns
# Define timeframes
timeframes = {
    '2-year': ('2023-03-01', '2025-03-01'),
    '5-year': ('2020-01-01', '2025-01-01'),
    '8-year': ('2017-01-01', '2025-01-01'),
    '10-year': ('2015-01-01', '2025-01-01')
}
# Example file paths – REPLACE these with your actual file paths
# Example file paths – REPLACE these with your actual file paths
file_paths = {
    'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
    'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
    'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
    'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
    'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
    'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
    'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
    'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
    'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
    'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
    'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
    'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
    # 'UNIONBANK': '/path/to/XOM.xlsx',
    'FIDELITYBANK': '/content/NSENG_FIDELITYBK, 1D_11cce.csv',
    'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
    'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
    'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
    'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
    'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
    'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
    # 'GSK': '/path/to/T.xlsx',
    'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
    'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
    'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
    'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
    'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
    'UPDC': '/content/NSENG_UPDC, 1D_58c00.csv',
    'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
    'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
    'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
    'PZ':'/content/NSENG_PZ, 1D_a1fb5.csv',
    'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
    'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv',
    'AIICO':'/content/NSENG_AIICO, 1D_71aa9 (1).csv',
    'AIRTEL':'/content/NSENG_AIRTELAFRI, 1D_1e3ed (1).csv',
    'ARADEL':'/content/NSENG_ARADEL, 1D_1b7ee (1).csv',
    'BUAFOOD':'/content/NSENG_BUAFOODS, 1D_4869e (1).csv',
    'CORNERSTONE':'/content/NSENG_CORNERST, 1D_745a2 (1).csv',
    'DANGSUGAR':'/content/NSENG_DANGSUGAR, 1D_601ba (1).csv',
    'ETI':'/content/NSENG_ETI, 1D_26f95.csv',
    'FBNH':'/content/NSENG_FBNH, 1D_15c8b (1).csv',
    'GEREGU':'/content/NSENG_GEREGU, 1D_ee307 (1).csv',
    'INTBREW':'/content/NSENG_INTBREW, 1D_84d73 (1).csv',
    'JAIZBANK':'/content/NSENG_JAIZBANK, 1D_bc6e8.csv',
    'MANSARD':'/content/NSENG_MANSARD, 1D_52cd5 (2).csv',
    'NB':'/content/NSENG_NB, 1D_45d75.csv',
    'OKOMUOIL':'/content/NSENG_OKOMUOIL, 1D_87901.csv',
    'OMATEK':'/content/NSENG_OMATEK, 1D_b5529 (1).csv',
    'PRESCO':'/content/NSENG_PRESCO, 1D_15e76 (1).csv',
    'STERLINGBANK':'/content/NSENG_STERLINGNG, 1D_ec3d0 (1).csv',
    'TIP':'/content/NSENG_TIP, 1D_b2cfb (1) (1).csv',
    'TRANSCORPHOTEL':'/content/NSENG_TRANSCOHOT, 1D_ed216.csv',
    'TRANSPOWER':'/content/NSENG_TRANSPOWER, 1D_081ff (1).csv',
    'UACN':'/content/NSENG_UACN, 1D_6163f (1).csv',
    'UCAP':'/content/NSENG_UCAP, 1D_181d9.csv',
    'UPDC':'/content/NSENG_UPDC, 1D_58c00 (1).csv',
    'WAPCO':'/content/NSENG_WAPCO, 1D_10d10 (1).csv',
    'WAPIC':'/content/NSENG_WAPIC, 1D_b83aa (2).csv',



}

# Step 2: Load and merge all data into a single DataFrame
def load_close_prices(file_paths):
    all_data = []
    for ticker, path in file_paths.items():
        try:
            # Check if the file exists before attempting to read it
            if os.path.exists(path):
                df = pd.read_excel(path) if path.endswith(".xlsx") else pd.read_csv(path)
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
                df = df[['close']].rename(columns={'close': ticker})
                all_data.append(df)
            else:
                print(f"File not found: {path} for ticker {ticker}") # Print a warning if the file is not found


        except Exception as e:
            print(f"Error reading {ticker}: {e}")

    # Check if all_data is empty before concatenating
    if all_data:
        merged = pd.concat(all_data, axis=1)
        merged.sort_index(inplace=True)
        return merged.dropna(axis=1, how='all')  # Drop tickers with all missing values
    else:
        print("No data loaded from any files. Check file paths and formats.")
        return pd.DataFrame()  # Return an empty DataFrame if no data is loaded


price_data = load_close_prices(file_paths)


# Print loaded tickers
print("\nLoaded Tickers:")
print(list(price_data.columns))


# Step 3: Calculate Correlation Table
def calculate_correlation(data, start_date, end_date):
    data_in_timeframe = data[(data.index >= start_date) & (data.index <= end_date)]
    return data_in_timeframe.corr()






correlation_tables = {}
for name, (start, end) in timeframes.items():
    start_date = pd.to_datetime(start)
    end_date = pd.to_datetime(end)
    correlation_tables[name] = calculate_correlation(price_data, start_date, end_date)


    # Create a separate Excel file for each time interval
    output_filename = f"{name}_correlation_table.xlsx"
    correlation_tables[name].to_excel(output_filename)
    print(f"\n{name} Correlation Table (saved to {output_filename}):")
    print(correlation_tables[name])
    correlation_table.to_excel("correlation_table.xlsx")


# Step 4: Cointegration Tests
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])


cointegration_results = perform_cointegration_tests(price_data)


print("\n=== Cointegration Results (p-value < 0.05) ===")
significant = cointegration_results[cointegration_results['P-Value'] < 0.05]
if not significant.empty:
    print(significant)
    significant.to_csv("cointegration_results.csv", index=False)
else:
    print("No significant cointegrated pairs found.")

correlation_table.to_excel("correlation_table.xlsx")

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
import os

# Define timeframes
timeframes = {
    '2-year': ('2023-03-01', '2025-03-01'),
    '5-year': ('2020-01-01', '2025-01-01'),
    '8-year': ('2017-01-01', '2025-01-01'),
    '10-year': ('2015-01-01', '2025-01-01')
}

# Your provided file_paths dictionary here
file_paths = {
    'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
    'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
    'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
    'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
    'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
    'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
    'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
    'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
    'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
    'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
    'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
    'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
    # 'UNIONBANK': '/path/to/XOM.xlsx',
    'FIDELITYBANK': '/content/NSENG_FIDELITYBK, 1D_11cce.csv',
    'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
    'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
    'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
    'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
    'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
    'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
    # 'GSK': '/path/to/T.xlsx',
    'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
    'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
    'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
    'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
    'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
    'UPDC': '/content/NSENG_UPDC, 1D_58c00.csv',
    'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
    'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
    'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
    'PZ':'/content/NSENG_PZ, 1D_a1fb5.csv',
    'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
    'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv',
    'AIICO':'/content/NSENG_AIICO, 1D_71aa9 (1).csv',
    'AIRTEL':'/content/NSENG_AIRTELAFRI, 1D_1e3ed (1).csv',
    'ARADEL':'/content/NSENG_ARADEL, 1D_1b7ee (1).csv',
    'BUAFOOD':'/content/NSENG_BUAFOODS, 1D_4869e (1).csv',
    'CORNERSTONE':'/content/NSENG_CORNERST, 1D_745a2 (1).csv',
    'DANGSUGAR':'/content/NSENG_DANGSUGAR, 1D_601ba (1).csv',
    'ETI':'/content/NSENG_ETI, 1D_26f95.csv',
    'FBNH':'/content/NSENG_FBNH, 1D_15c8b (1).csv',
    'GEREGU':'/content/NSENG_GEREGU, 1D_ee307 (1).csv',
    'INTBREW':'/content/NSENG_INTBREW, 1D_84d73 (1).csv',
    'JAIZBANK':'/content/NSENG_JAIZBANK, 1D_bc6e8.csv',
    'MANSARD':'/content/NSENG_MANSARD, 1D_52cd5 (2).csv',
    'NB':'/content/NSENG_NB, 1D_45d75.csv',
    'OKOMUOIL':'/content/NSENG_OKOMUOIL, 1D_87901.csv',
    'OMATEK':'/content/NSENG_OMATEK, 1D_b5529 (1).csv',
    'PRESCO':'/content/NSENG_PRESCO, 1D_15e76 (1).csv',
    'STERLINGBANK':'/content/NSENG_STERLINGNG, 1D_ec3d0 (1).csv',
    'TIP':'/content/NSENG_TIP, 1D_b2cfb (1) (1).csv',
    'TRANSCORPHOTEL':'/content/NSENG_TRANSCOHOT, 1D_ed216.csv',
    'TRANSPOWER':'/content/NSENG_TRANSPOWER, 1D_081ff (1).csv',
    'UACN':'/content/NSENG_UACN, 1D_6163f (1).csv',
    'UCAP':'/content/NSENG_UCAP, 1D_181d9.csv',
    'UPDC':'/content/NSENG_UPDC, 1D_58c00 (1).csv',
    'WAPCO':'/content/NSENG_WAPCO, 1D_10d10 (1).csv',
    'WAPIC':'/content/NSENG_WAPIC, 1D_b83aa (2).csv',



}


# Load close prices from multiple files
def load_close_prices(file_paths):
    all_data = []
    for ticker, path in file_paths.items():
        try:
            if os.path.exists(path):
                df = pd.read_excel(path) if path.endswith(".xlsx") else pd.read_csv(path)
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
                df = df[['close']].rename(columns={'close': ticker})
                all_data.append(df)
            else:
                print(f"File not found: {path} for ticker {ticker}")
        except Exception as e:
            print(f"Error reading {ticker}: {e}")
    if all_data:
        merged = pd.concat(all_data, axis=1)
        merged.sort_index(inplace=True)
        return merged.dropna(axis=1, how='all')
    else:
        print("No data loaded from any files.")
        return pd.DataFrame()

# Load all price data
price_data = load_close_prices(file_paths)

print("\nLoaded Tickers:")
print(list(price_data.columns))

# Calculate correlation
def calculate_correlation(data, start_date, end_date):
    data_in_timeframe = data[(data.index >= start_date) & (data.index <= end_date)]
    return data_in_timeframe.corr()

# Step 3: Correlation Tables for Each Timeframe
correlation_tables = {}
for name, (start, end) in timeframes.items():
    start_date = pd.to_datetime(start)
    end_date = pd.to_datetime(end)
    correlation_table = calculate_correlation(price_data, start_date, end_date)
    correlation_tables[name] = correlation_table

    output_filename = f"{name}_correlation_table.xlsx"
    correlation_table.to_excel(output_filename)
    print(f"\n{name} Correlation Table saved as: {output_filename}")
    print(correlation_table)

# Step 4: Cointegration Tests
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1].dropna(), data[asset2].dropna())
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

# Run and save cointegration results
cointegration_results = perform_cointegration_tests(price_data)
significant = cointegration_results[cointegration_results['P-Value'] < 0.05]

print("\n=== Cointegration Results (p-value < 0.05) ===")
if not significant.empty:
    print(significant)
    significant.to_csv("cointegration_results.csv", index=False)
    print("Significant cointegration results saved to 'cointegration_results.csv'")
else:
    print("No significant cointegrated pairs found.")

# Step 4: Enhanced Cointegration Tests with Recommendations
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                series1 = data[asset1].dropna()
                series2 = data[asset2].dropna()
                joined = pd.concat([series1, series2], axis=1).dropna()
                score, pvalue, _ = coint(joined.iloc[:, 0], joined.iloc[:, 1])

                # Interpret result
                if pvalue < 0.05:
                    recommendation = "Highly Cointegrated ✅"
                elif pvalue < 0.1:
                    recommendation = "Moderate Cointegration ⚠️"
                else:
                    recommendation = "Not Cointegrated ❌"

                results.append((asset1, asset2, score, pvalue, recommendation))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=[
        'Asset1', 'Asset2', 'Cointegration Score', 'P-Value', 'Recommendation'
    ])

# Run tests
cointegration_df = perform_cointegration_tests(price_data)

# Save to Excel with multiple sheets
with pd.ExcelWriter("cointegration_results.xlsx") as writer:
    cointegration_df.to_excel(writer, sheet_name="All Results", index=False)

    # Filter significant pairs
    top_pairs = cointegration_df[cointegration_df['P-Value'] < 0.05].sort_values("P-Value")
    top_pairs.to_excel(writer, sheet_name="Top Pairs", index=False)

print("\nCointegration results saved to 'cointegration_results.xlsx'")

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
import os

# Step 1: Timeframes
timeframes = {
    '2-year': ('2023-03-01', '2025-03-01'),
    '5-year': ('2020-01-01', '2025-01-01'),
    '8-year': ('2017-01-01', '2025-01-01'),
    '10-year': ('2015-01-01', '2025-01-01')
}

# Your provided file_paths dictionary here
file_paths = {
    'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
    'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
    'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
    'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
    'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
    'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
    'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
    'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
    'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
    'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
    'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
    'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
    # 'UNIONBANK': '/path/to/XOM.xlsx',
    'FIDELITYBANK': '/content/NSENG_FIDELITYBK, 1D_11cce.csv',
    'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
    'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
    'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
    'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
    'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
    'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
    # 'GSK': '/path/to/T.xlsx',
    'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
    'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
    'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
    'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
    'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
    'UPDC': '/content/NSENG_UPDC, 1D_58c00.csv',
    'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
    'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
    'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
    'PZ':'/content/NSENG_PZ, 1D_a1fb5.csv',
    'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
    'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv',
    'AIICO':'/content/NSENG_AIICO, 1D_71aa9 (1).csv',
    'AIRTEL':'/content/NSENG_AIRTELAFRI, 1D_1e3ed (1).csv',
    'ARADEL':'/content/NSENG_ARADEL, 1D_1b7ee (1).csv',
    'BUAFOOD':'/content/NSENG_BUAFOODS, 1D_4869e (1).csv',
    'CORNERSTONE':'/content/NSENG_CORNERST, 1D_745a2 (1).csv',
    'DANGSUGAR':'/content/NSENG_DANGSUGAR, 1D_601ba (1).csv',
    'ETI':'/content/NSENG_ETI, 1D_26f95.csv',
    'FBNH':'/content/NSENG_FBNH, 1D_15c8b (1).csv',
    'GEREGU':'/content/NSENG_GEREGU, 1D_ee307 (1).csv',
    'INTBREW':'/content/NSENG_INTBREW, 1D_84d73 (1).csv',
    'JAIZBANK':'/content/NSENG_JAIZBANK, 1D_bc6e8.csv',
    'MANSARD':'/content/NSENG_MANSARD, 1D_52cd5 (2).csv',
    'NB':'/content/NSENG_NB, 1D_45d75.csv',
    'OKOMUOIL':'/content/NSENG_OKOMUOIL, 1D_87901.csv',
    'OMATEK':'/content/NSENG_OMATEK, 1D_b5529 (1).csv',
    'PRESCO':'/content/NSENG_PRESCO, 1D_15e76 (1).csv',
    'STERLINGBANK':'/content/NSENG_STERLINGNG, 1D_ec3d0 (1).csv',
    'TIP':'/content/NSENG_TIP, 1D_b2cfb (1) (1).csv',
    'TRANSCORPHOTEL':'/content/NSENG_TRANSCOHOT, 1D_ed216.csv',
    'TRANSPOWER':'/content/NSENG_TRANSPOWER, 1D_081ff (1).csv',
    'UACN':'/content/NSENG_UACN, 1D_6163f (1).csv',
    'UCAP':'/content/NSENG_UCAP, 1D_181d9.csv',
    'UPDC':'/content/NSENG_UPDC, 1D_58c00 (1).csv',
    'WAPCO':'/content/NSENG_WAPCO, 1D_10d10 (1).csv',
    'WAPIC':'/content/NSENG_WAPIC, 1D_b83aa (2).csv',



}

# Step 3: Load and convert prices to returns
def load_return_data(file_paths):
    all_data = []
    for ticker, path in file_paths.items():
        try:
            if os.path.exists(path):
                df = pd.read_excel(path) if path.endswith(".xlsx") else pd.read_csv(path)
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
                df = df[['close']].rename(columns={'close': ticker})
                df[ticker] = np.log(df[ticker] / df[ticker].shift(1))  # Log returns
                all_data.append(df)
            else:
                print(f"File not found: {path} for ticker {ticker}")
        except Exception as e:
            print(f"Error reading {ticker}: {e}")

    if all_data:
        merged = pd.concat(all_data, axis=1)
        merged.sort_index(inplace=True)
        return merged.dropna(how='all')
    else:
        print("No data loaded.")
        return pd.DataFrame()

returns_data = load_return_data(file_paths)

# Print tickers successfully loaded
print("\nLoaded Tickers with Returns:")
print(list(returns_data.columns))

# Step 4: Correlation on returns
def calculate_correlation(data, start_date, end_date):
    data_in_timeframe = data[(data.index >= start_date) & (data.index <= end_date)]
    return data_in_timeframe.corr()

correlation_tables = {}
for name, (start, end) in timeframes.items():
    start_date = pd.to_datetime(start)
    end_date = pd.to_datetime(end)
    corr_df = calculate_correlation(returns_data, start_date, end_date)
    correlation_tables[name] = corr_df
    output_filename = f"{name}_correlation_table.xlsx"
    corr_df.to_excel(output_filename)
    print(f"\n{name} Correlation Table saved to {output_filename}:")
    print(corr_df)

# Step 5: Cointegration of returns
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                series1 = data[asset1].dropna()
                series2 = data[asset2].dropna()
                joined = pd.concat([series1, series2], axis=1).dropna()
                score, pvalue, _ = coint(joined.iloc[:, 0], joined.iloc[:, 1])
                if pvalue < 0.05:
                    recommendation = "Highly Cointegrated ✅"
                elif pvalue < 0.1:
                    recommendation = "Moderate Cointegration ⚠️"
                else:
                    recommendation = "Not Cointegrated ❌"
                results.append((asset1, asset2, score, pvalue, recommendation))
            except Exception as e:
                print(f"Error for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=[
        'Asset1', 'Asset2', 'Cointegration Score', 'P-Value', 'Recommendation'
    ])

cointegration_df = perform_cointegration_tests(returns_data)

# Save cointegration results to Excel
with pd.ExcelWriter("cointegration_results.xlsx") as writer:
    cointegration_df.to_excel(writer, sheet_name="All Results", index=False)
    top_pairs = cointegration_df[cointegration_df['P-Value'] < 0.05].sort_values("P-Value")
    top_pairs.to_excel(writer, sheet_name="Top Pairs", index=False)

print("\n=== Cointegration Results (Saved to cointegration_results.xlsx) ===")
if not top_pairs.empty:
    print(top_pairs)
else:
    print("No strongly cointegrated return pairs found.")

import pandas as pd
import numpy as np
from statsmodels.tsa.vector_ar.vecm import coint_johansen
from scipy.stats import zscore

def load_data(file_paths):
    """
    Load OHLCV data from multiple CSV/Excel files.
    """
    all_data = []
    for ticker, path in file_paths.items():
        try:
            if path.endswith('.csv'):
                df = pd.read_csv(path)
            elif path.endswith('.xlsx'):
                df = pd.read_excel(path)
            else:
                print(f"Unsupported file format for {path}")
                continue

            # Ensure required columns exist
            if 'time' not in df.columns or 'close' not in df.columns or 'volume' not in df.columns:
                print(f"Missing required columns in {path}. Skipping.")
                continue

            # Format the data
            df['time'] = pd.to_datetime(df['time'])
            df.set_index('time', inplace=True)
            df = df[['close', 'volume']].rename(columns={'close': ticker, 'volume': f'{ticker}_volume'})
            all_data.append(df)
        except Exception as e:
            print(f"Error loading {path}: {e}")

    if not all_data:
        print("No valid data loaded.")
        return pd.DataFrame()

    # Merge all data into a single DataFrame
    merged_data = pd.concat(all_data, axis=1)
    merged_data.sort_index(inplace=True)
    return merged_data.dropna(axis=1, how='all')

def calculate_volatility(data):
    """
    Calculate daily volatility for each asset.
    """
    volatility = data.pct_change().std() * np.sqrt(252)  # Annualized volatility
    return volatility

def calculate_liquidity(data):
    """
    Calculate average daily trading volume as a proxy for liquidity.
    """
    liquidity = data.filter(like='_volume').mean()
    return liquidity

def johansen_cointegration_test(data, significance_level=0.05):
    """
    Perform Johansen cointegration test on the given data.
    """
    result = coint_johansen(data, det_order=0, k_ar_diff=1)
    trace_stat = result.lr1  # Trace statistic
    critical_values = result.cvt[:, 0]  # Critical values at 90%, 95%, 99%

    # Check if the trace statistic exceeds the critical value at the given significance level
    is_cointegrated = trace_stat[0] > critical_values[1]  # 95% confidence level
    return is_cointegrated, trace_stat[0], critical_values[1]

def recommend_tradeable_pairs(file_paths, min_volatility=0.1, min_liquidity=1e6):
    """
    Recommend tradeable pairs based on volatility, liquidity, and cointegration.
    """
    # Step 1: Load data
    data = load_data(file_paths)
    if data.empty:
        return []

    # Separate close prices and volumes
    close_prices = data.filter(regex='^(?!.*_volume$)')  # Exclude volume columns
    volumes = data.filter(like='_volume')

    # Step 2: Calculate volatility and liquidity
    volatility = calculate_volatility(close_prices)
    liquidity = calculate_liquidity(volumes)

    # Step 3: Filter assets based on minimum volatility and liquidity
    eligible_assets = close_prices.columns[
        (volatility >= min_volatility) & (liquidity >= min_liquidity)
    ]
    filtered_data = close_prices[eligible_assets]

    if len(eligible_assets) < 2:
        print("Not enough eligible assets to form pairs.")
        return []

    # Step 4: Test for cointegration among all pairs
    tradeable_pairs = []
    n = len(eligible_assets)
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = eligible_assets[i]
            asset2 = eligible_assets[j]
            pair_data = filtered_data[[asset1, asset2]].dropna()

            if len(pair_data) < 100:  # Minimum data points for reliable results
                continue

            # Perform Johansen cointegration test
            is_cointegrated, trace_stat, critical_value = johansen_cointegration_test(pair_data)
            if is_cointegrated:
                tradeable_pairs.append({
                    'Pair': (asset1, asset2),
                    'Trace Statistic': trace_stat,
                    'Critical Value': critical_value,
                    'Volatility': (volatility[asset1], volatility[asset2]),
                    'Liquidity': (liquidity[asset1], liquidity[asset2])
                })

    # Step 5: Sort pairs by trace statistic (higher is better)
    tradeable_pairs.sort(key=lambda x: x['Trace Statistic'], reverse=True)
    return tradeable_pairs

# Example usage
if __name__ == "__main__":
    # Define file paths for each ticker
    file_paths = {
        # Your provided file_paths dictionary here

    'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
    'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
    'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
    'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
    'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
    'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
    'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
    'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
    'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
    'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
    'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
    'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
    'FIDELITYBANK': '/content/NSENG_FIDELITYBK, 1D_11cce.csv',
    'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
    'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
    'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
    'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
    'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
    'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
    'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
    'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
    'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
    'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
    'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
    'UPDC': '/content/NSENG_UPDC, 1D_58c00.csv',
    'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
    'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
    'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
    'PZ':'/content/NSENG_PZ, 1D_a1fb5.csv',
    'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
    'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv',
    'AIICO':'/content/NSENG_AIICO, 1D_71aa9 (1).csv',
    'AIRTEL':'/content/NSENG_AIRTELAFRI, 1D_1e3ed (1).csv',
    'ARADEL':'/content/NSENG_ARADEL, 1D_1b7ee (1).csv',
    'BUAFOOD':'/content/NSENG_BUAFOODS, 1D_4869e (1).csv',
    'CORNERSTONE':'/content/NSENG_CORNERST, 1D_745a2 (1).csv',
    'DANGSUGAR':'/content/NSENG_DANGSUGAR, 1D_601ba (1).csv',
    'ETI':'/content/NSENG_ETI, 1D_26f95.csv',
    'FBNH':'/content/NSENG_FBNH, 1D_15c8b (1).csv',
    'GEREGU':'/content/NSENG_GEREGU, 1D_ee307 (1).csv',
    'INTBREW':'/content/NSENG_INTBREW, 1D_84d73 (1).csv',
    'JAIZBANK':'/content/NSENG_JAIZBANK, 1D_bc6e8.csv',
    'MANSARD':'/content/NSENG_MANSARD, 1D_52cd5 (2).csv',
    'NB':'/content/NSENG_NB, 1D_45d75.csv',
    'OKOMUOIL':'/content/NSENG_OKOMUOIL, 1D_87901.csv',
    'OMATEK':'/content/NSENG_OMATEK, 1D_b5529 (1).csv',
    'PRESCO':'/content/NSENG_PRESCO, 1D_15e76 (1).csv',
    'STERLINGBANK':'/content/NSENG_STERLINGNG, 1D_ec3d0 (1).csv',
    'TIP':'/content/NSENG_TIP, 1D_b2cfb (1) (1).csv',
    'TRANSCORPHOTEL':'/content/NSENG_TRANSCOHOT, 1D_ed216.csv',
    'TRANSPOWER':'/content/NSENG_TRANSPOWER, 1D_081ff (1).csv',
    'UACN':'/content/NSENG_UACN, 1D_6163f (1).csv',
    'UCAP':'/content/NSENG_UCAP, 1D_181d9.csv',
    'UPDC':'/content/NSENG_UPDC, 1D_58c00 (1).csv',
    'WAPCO':'/content/NSENG_WAPCO, 1D_10d10 (1).csv',
    'WAPIC':'/content/NSENG_WAPIC, 1D_b83aa (2).csv',

    }

    # Recommend tradeable pairs
    recommended_pairs = recommend_tradeable_pairs(
        file_paths,
        min_volatility=0.1,  # Minimum annualized volatility
        min_liquidity=1e6    # Minimum average daily trading volume
    )

    # Print results
    if recommended_pairs:
        print("Recommended Tradeable Pairs:")
        for pair in recommended_pairs:
            print(f"Pair: {pair['Pair']}")
            print(f"  Trace Statistic: {pair['Trace Statistic']:.2f}, Critical Value: {pair['Critical Value']:.2f}")
            print(f"  Volatility: {pair['Volatility']}")
            print(f"  Liquidity: {pair['Liquidity']}")
    else:
        print("No tradeable pairs found.")

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
from statsmodels.tsa.vector_ar.vecm import VECM
from scipy.stats import norm
import matplotlib.pyplot as plt

# Load data from CSV/Excel files
def load_data(file_path):
    if file_path.endswith('.csv'):
        data = pd.read_csv(file_path)
    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
        data = pd.read_excel(file_path)
    elif file_path.endswith('.txt'):
        data = pd.read_csv(file_path, delimiter='\t')  # Assuming tab-delimited
    else:
        raise ValueError('Unsupported file format')

    # Normalize column names
    data.columns = [col.strip().lower().replace(' ', '_') for col in data.columns]

    # Adjust expected columns
    column_map = {
        'open_price': 'open',
        'high_price': 'high',
        'low_price': 'low',
        'close_price': 'close',
        'closing_price': 'close',
        'volume_traded': 'volume',
        'volume_(units)': 'volume'
    }
    data.rename(columns=column_map, inplace=True)

    # Required columns check
    required_columns = ['time', 'open', 'high', 'low', 'close', 'volume']
    if not all(col in data.columns for col in required_columns):
        raise ValueError('Data missing required columns')

    # Convert Time column to datetime
    data['time'] = pd.to_datetime(data['time'], errors='coerce')
    data = data.dropna(subset=['time'])
    data.set_index('time', inplace=True)

    # Add this line to make the index timezone-naive
    data.index = data.index.tz_localize(None)

    return data

# Calculate volatility
def calculate_volatility(data):
    return data['close'].pct_change().std() * np.sqrt(252)

# Calculate liquidity
def calculate_liquidity(data):
    return data['volume'].mean()

# Perform Johansen cointegration test
def johansen_test(data1, data2):
    combined = pd.concat([data1['close'], data2['close']], axis=1).dropna()
    if len(combined) < 100:
        return None, None  # Not enough data
    try:
        model = VECM(endog=combined.values, k_ar_diff=1, coint_rank=1)
        result = model.fit()
        test_stat = result.test_coint_rank().statistic
        p_value = result.test_coint_rank().pvalue
        return test_stat, p_value
    except Exception as e:
        return None, None

# Find cointegrated pairs
def find_cointegrated_pairs(data_list):
    pairs = []
    for i in range(len(data_list)):
        for j in range(i + 1, len(data_list)):
            data1 = data_list[i]
            data2 = data_list[j]

            vol1 = calculate_volatility(data1)
            vol2 = calculate_volatility(data2)
            liq1 = calculate_liquidity(data1)
            liq2 = calculate_liquidity(data2)
            test_stat, p_value = johansen_test(data1, data2)

            # Check if p_value is not None before proceeding
            if p_value is not None:
                if p_value < 0.05 or vol1 > 0.1 or vol2 > 0.1 and liq1 > 100000 and liq2 > 100000:
                    pairs.append((data1.name, data2.name, test_stat, p_value, vol1, vol2, liq1, liq2))
    return pairs

# Main function
def main():
    file_paths = {
        'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
        'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
        'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
        'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
        'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
        'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
        'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
        'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
        'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
        'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
        'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
        'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
        'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
        'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
        'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
        'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
        'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
        'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
        'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
        'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
        'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
        'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
        'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
        'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
        'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
        'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
        'PZ': '/content/NSENG_PZ, 1D_a1fb5.csv',
        'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
        'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv',
        'AIRTEL': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
        'BUAFOOD': '/content/NSENG_BUAFOODS, 1D_4869e (1).csv',
        'CORNERSTONE': '/content/NSENG_CORNERST, 1D_745a2.csv',
        'DANGSUGAR': '/content/NSENG_CORNERST, 1D_745a2.csv',
        'GEREGU': '/content/NSENG_GEREGU, 1D_ee307.csv',
        'JAIZBANK': '/content/NSENG_JAIZBANK, 1D_bc6e8.csv',
        'MANSARD': '/content/NSENG_MANSARD, 15_031bc.csv',
        'NB': '/content/NSENG_NB, 1D_45d75.csv',
        'OKOMUOIL': '/content/NSENG_OKOMUOIL, 1D_87901.csv',
        'TRANSCORPHOTEL': '/content/NSENG_TRANSCOHOT, 1D_ed216.csv',
        'UCAP': '/content/NSENG_UCAP, 1D_181d9.csv',
    }

    data_list = []
    for ticker, file_path in file_paths.items():
        try:
            df = load_data(file_path)
            df.name = ticker
            data_list.append(df)
        except ValueError as e:
            print(f"Skipping {ticker} due to error: {e}")

    pairs = find_cointegrated_pairs(data_list)

    print("\n=== Cointegrated Pairs ===")
    for pair in pairs:
        print(f'Pair: {pair[0]} and {pair[1]}')

        # Check if test_stat is None before formatting
        if pair[2] is not None:
            print(f'  Test Statistic: {pair[2]:.2f}')
        else:
            print(f'  Test Statistic: Could not be calculated')  # Handle None case

        # Check if p_value is None before formatting
        if pair[3] is not None:
            print(f'  p-value: {pair[3]:.4f}')
        else:
            print(f'  p-value: Could not be calculated')  # Handle None case

        print(f'  Volatility: {pair[4]:.4f} vs {pair[5]:.4f}')
        print(f'  Liquidity: {pair[6]:,.0f} vs {pair[7]:,.0f}')
        print('------------------------')
if __name__ == '__main__':
    main()

!pip install arch --upgrade
!pip install pykalman --upgrade
# Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.vector_ar.vecm import coint_johansen
from statsmodels.tsa.vector_ar.var_model import VAR
from pykalman import KalmanFilter
from arch.unitroot import ADF

# 1. Load GTCO and UBA Data
gtco_df = pd.read_csv("/content/NSENG_GTCO, 1D_f22f4.csv", parse_dates=["time"], index_col="time")
uba_df = pd.read_csv("/content/NSENG_UBA, 1D_4a819.csv", parse_dates=["time"], index_col="time")

gtco_df.rename(columns={"close": "GTCO"}, inplace=True)
uba_df.rename(columns={"close": "UBA"}, inplace=True)

merged_df = pd.merge(gtco_df[["GTCO"]], uba_df[["UBA"]], left_index=True, right_index=True).dropna()
log_df = np.log(merged_df)

# 2. Rolling Johansen Cointegration Test
rolling_window = 120
rolling_trace_stats = []
rolling_dates = []

for i in range(len(log_df) - rolling_window):
    window_data = log_df.iloc[i:i+rolling_window]
    result = coint_johansen(window_data, det_order=0, k_ar_diff=1)
    rolling_trace_stats.append(result.lr1[0])  # rank 0 trace stat
    rolling_dates.append(log_df.index[i + rolling_window])

# 3. Kalman Filter Estimation of Dynamic Hedge Ratio
y = merged_df['GTCO'].values
x = merged_df['UBA'].values

# Reshape x to be a column vector for consistency
x = x.reshape(-1, 1)

# Modify the observation matrix to be time-varying
observation_matrices = np.hstack([np.ones((len(x), 1)), x])

kf = KalmanFilter(
    transition_matrices=np.eye(2),
    # observation_matrices=observation_matrices,  # Use the time-varying observation matrix
    initial_state_mean=np.zeros(2),
    initial_state_covariance=np.ones((2, 2)),
    observation_covariance=1.0,
    transition_covariance=0.01 * np.eye(2),
    n_dim_obs=1  # Specify the number of observed dimensions
)

# Use the filter method with the time-varying observation matrices
state_means, _ = kf.filter_update(
    kf.initial_state_mean, kf.initial_state_covariance,
    observation_matrices=observation_matrices,
    observation_offsets=np.zeros(len(observation_matrices)),
    observations=y
)

kalman_beta = state_means[:, 1]

# 4. Gregory-Hansen Cointegration Test
# (GregoryHansen import and usage unchanged)

# 5. Plotting Results
# (Plotting code unchanged)

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import coint
import os

# Step 1: Read 20 user-specified files containing 'Date' and 'Close' columns
# Define timeframes
timeframes = {
    '2-year': ('2023-03-01', '2025-03-01'),
    '5-year': ('2020-01-01', '2025-01-01'),
    '8-year': ('2017-01-01', '2025-01-01'),
    '10-year': ('2015-01-01', '2025-01-01')
}

# Example file paths – REPLACE these with your actual file paths
file_paths = {
    'GTCO': '/content/NSENG_GTCO, 1D_e2363.csv',
    'UBA': '/content/NSENG_UBA, 1D_d298c.csv',
    'ZENITHBANK': '/content/NSENG_ZENITHBANK, 1D_6d9c1.csv',
    'MTNN': '/content/NSENG_MTNN, 1D_ed527.csv',
    'AIRTELAFRICA': '/content/NSENG_AIRTELAFRI, 1D_1e3ed.csv',
    'DANGCEM': '/content/NSENG_DANGCEM, 1D_bbb2a.csv',
    'BUACEMENT': '/content/NSENG_BUACEMENT, 1D_9928b.csv',
    'UNILEVER': '/content/NSENG_UNILEVER, 1D_255fa.csv',
    'NESTLE': '/content/NSENG_NESTLE, 1D_7b085.csv',
    'FBN': '/content/NSENG_FBNH, 1D_15c8b.csv',
    'FCMB': '/content/NSENG_FCMB, 1D_046f3.csv',
    'ACCESSBANK': '/content/NSENG_ACCESSCORP, 1D_71fee.csv',
    'FIDELITYBANK': '/content/NSENG_FIDELITYBK, 1D_11cce.csv',
    'ETI': '/content/NSENG_ETI, 1D_26f95.csv',
    'GUINESS': '/content/NSENG_GUINNESS, 1D_f13f7.csv',
    'FLOURMILLS': '/content/NSENG_FLOURMILL, 1D_ad9dd.csv',
    'CONOIL': '/content/NSENG_CONOIL, 1D_61bad.csv',
    'OANDO': '/content/NSENG_OANDO, 1D_b0fdc.csv',
    'SEPLAT': '/content/NSENG_SEPLAT, 1D_b2172.csv',
    'MAYANDBAKER': '/content/NSENG_MAYBAKER, 1D_bb971.csv',
    'JULIUSBERGER': '/content/NSENG_JBERGER, 1D_20fac.csv',
    'JOHNHOLT': '/content/NSENG_JOHNHOLT, 1D_a5aa9.csv',
    'UAC': '/content/NSENG_UACN, 1D_6163f.csv',
    'TRANSCORP': '/content/NSENG_TRANSCORP, 1D_62ab7.csv',
    'UPDC': '/content/NSENG_UPDC, 1D_58c00.csv',
    'IKEJAHOTEL': '/content/NSENG_IKEJAHOTEL, 1D_8db9f.csv',
    'ETERNA': '/content/NSENG_ETERNA, 1D_d1021.csv',
    'OMATEK': '/content/NSENG_OMATEK, 1D_b5529.csv',
    'PZ': '/content/NSENG_PZ, 1D_a1fb5.csv',
    'STANBICIBTC': '/content/NSENG_STANBIC, 1D_da76f.csv',
    'TOTALENERGIES': '/content/NSENG_TOTAL, 1D_9ee25.csv'
}

# Step 2: Load and merge all data into a single DataFrame
def load_close_prices(file_paths):
    all_data = []
    for ticker, path in file_paths.items():
        try:
            # Check if the file exists before attempting to read it
            if os.path.exists(path):
                df = pd.read_excel(path) if path.endswith(".xlsx") else pd.read_csv(path)
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
                df = df[['close']].rename(columns={'close': ticker})
                all_data.append(df)
            else:
                print(f"File not found: {path} for ticker {ticker}")  # Print a warning if the file is not found

        except Exception as e:
            print(f"Error reading {ticker}: {e}")

    # Check if all_data is empty before concatenating
    if all_data:
        merged = pd.concat(all_data, axis=1)
        merged.sort_index(inplace=True)
        return merged.dropna(axis=1, how='all')  # Drop tickers with all missing values
    else:
        print("No data loaded from any files. Check file paths and formats.")
        return pd.DataFrame()  # Return an empty DataFrame if no data is loaded


price_data = load_close_prices(file_paths)

# Convert daily prices to monthly prices
def resample_to_monthly(data):
    """
    Resamples daily data to monthly data using the last trading day's closing price.
    """
    return data.resample('3M').last()

monthly_price_data = resample_to_monthly(price_data)

# Print loaded tickers
print("\nLoaded Tickers:")
print(list(monthly_price_data.columns))

# Step 3: Calculate Correlation Table
def calculate_correlation(data, start_date, end_date):
    data_in_timeframe = data[(data.index >= start_date) & (data.index <= end_date)]
    return data_in_timeframe.corr()


correlation_tables = {}
for name, (start, end) in timeframes.items():
    start_date = pd.to_datetime(start)
    end_date = pd.to_datetime(end)
    correlation_tables[name] = calculate_correlation(monthly_price_data, start_date, end_date)

    # Create a separate Excel file for each time interval
    output_filename = f"{name}_correlation_table.xlsx"
    correlation_tables[name].to_excel(output_filename)
    print(f"\n{name} Correlation Table (saved to {output_filename}):")
    print(correlation_tables[name])

# Step 4: Cointegration Tests
def perform_cointegration_tests(data):
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])


cointegration_results = perform_cointegration_tests(monthly_price_data)

print("\n=== Cointegration Results (p-value < 0.05) ===")
significant = cointegration_results[cointegration_results['P-Value'] < 0.05]
if not significant.empty:
    print(significant)
    significant.to_csv("cointegration_results.csv", index=False)
else:
    print("No significant cointegrated pairs found.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import coint
from statsmodels.tsa.vector_ar.vecm import coint_johansen
import os

def load_and_prepare_data(filepath, ticker_name):
    """
    Load a CSV file containing OHLCV data, extract the 'Date' and 'Close' columns,
    set the date as index, rename the 'Close' column to include ticker_name, and filter date range.
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")

    df = pd.read_csv(filepath, parse_dates=['time'])
    df = df[['time', 'close']].dropna()
    df = df.rename(columns={'close': f'{ticker_name}_close'})
    df.set_index('time', inplace=True)
    df = df['2015-01-01':'2025-12-31']
    return df

def perform_engle_granger_test(series1, series2):
    """
    Perform Engle-Granger 2-step cointegration test.
    """
    print("\n--- Engle-Granger Two-Step Cointegration Test ---")
    score, pvalue, _ = coint(series1, series2)
    print(f"Test Statistic: {score:.4f}")
    print(f"P-value: {pvalue:.4f}")
    if pvalue < 0.05:
        print("✅ Series are likely cointegrated (reject null hypothesis of no cointegration).")
    else:
        print("❌ No evidence of cointegration (fail to reject null).")

def perform_johansen_test(df):
    """
    Perform Johansen Cointegration Test.
    """
    print("\n--- Johansen Cointegration Test ---")
    johansen = coint_johansen(df, det_order=0, k_ar_diff=1)

    trace_stat = johansen.lr1
    crit_vals = johansen.cvt

    for i in range(len(trace_stat)):
        print(f"\nRank {i}:")
        print(f"Trace Statistic: {trace_stat[i]:.4f}")
        print(f"Critical Values (90%, 95%, 99%): {crit_vals[i]}")
        if trace_stat[i] > crit_vals[i][1]:
            print("✅ Evidence of cointegration at 5% level.")
        else:
            print("❌ No cointegration at 5% level.")

def plot_series(df):
    """
    Plot the two closing price series.
    """
    plt.figure(figsize=(14, 6))
    plt.plot(df.index, df.iloc[:, 0], label=df.columns[0])
    plt.plot(df.index, df.iloc[:, 1], label=df.columns[1])
    plt.title('UBA vs GTCO Closing Prices (2015–2025)', fontsize=14)
    plt.xlabel('time')
    plt.ylabel('Closing Price (NGN)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def run_cointegration_pipeline(uba_file_path, gtco_file_path):
    """
    Load data, perform plotting and both cointegration tests.
    """
    # Load data
    uba = load_and_prepare_data(uba_file_path, 'UBA')
    gtco = load_and_prepare_data(gtco_file_path, 'GTCO')

    # Merge datasets on Date
    df_combined = pd.merge(uba, gtco, left_index=True, right_index=True).dropna()

    # Plot time series
    plot_series(df_combined)

    # Perform Engle-Granger Test
    perform_engle_granger_test(df_combined['UBA_close'], df_combined['GTCO_close'])

    # Perform Johansen Test
    perform_johansen_test(df_combined)

def main():
    """
    Main entry point of the script.
    Replace the file paths with the actual location of your UBA and GTCO CSV files.
    """
    uba_file = "/content/NSENG_UBA, 1D_3989d.csv"     # Replace with your actual path
    gtco_file = "/content/NSENG_GTCO, 1D_92904.csv"   # Replace with your actual path

    try:
        run_cointegration_pipeline(uba_file, gtco_file)
    except Exception as e:
        print(f"Error: {e}")

# This ensures main() only runs when the script is directly executed
if __name__ == "__main__":
    main()

uba_log = np.log(df['UBA_Close'])
gtco_log = np.log(df['GTCO_Close'])

score, pvalue, _ = coint(uba_log, gtco_log)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import coint
from statsmodels.tsa.vector_ar.vecm import coint_johansen
import os

def load_and_prepare_data(filepath, ticker_name):
    """
    Load CSV OHLCV data, extract 'time' and 'close', rename columns, filter 2015–2025.
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")

    df = pd.read_csv(filepath, parse_dates=['time'])
    df = df[['time', 'close']].dropna()
    df = df.rename(columns={'close': f'{ticker_name}_close'})
    df.set_index('time', inplace=True)
    df = df['2015-01-01':'2025-12-31']
    return df

def perform_engle_granger_test(series1, series2, label=''):
    """
    Perform Engle-Granger 2-step cointegration test.
    """
    print(f"\n--- Engle-Granger Test {label} ---")
    score, pvalue, _ = coint(series1, series2)
    print(f"Test Statistic: {score:.4f}")
    print(f"P-value: {pvalue:.4f}")
    if pvalue < 0.05:
        print("✅ Likely cointegrated (reject null hypothesis of no cointegration).")
    else:
        print("❌ No evidence of cointegration (fail to reject null).")

def perform_johansen_test(df, label=''):
    """
    Perform Johansen cointegration test.
    """
    print(f"\n--- Johansen Test {label} ---")
    johansen = coint_johansen(df, det_order=0, k_ar_diff=1)

    trace_stat = johansen.lr1
    crit_vals = johansen.cvt

    for i in range(len(trace_stat)):
        print(f"\nRank {i}:")
        print(f"Trace Statistic: {trace_stat[i]:.4f}")
        print(f"Critical Values (90%, 95%, 99%): {crit_vals[i]}")
        if trace_stat[i] > crit_vals[i][1]:
            print("✅ Cointegration detected at 5% level.")
        else:
            print("❌ No cointegration at 5% level.")

def plot_series(df, title):
    """
    Plot two series.
    """
    plt.figure(figsize=(14, 6))
    plt.plot(df.index, df.iloc[:, 0], label=df.columns[0])
    plt.plot(df.index, df.iloc[:, 1], label=df.columns[1])
    plt.title(title, fontsize=14)
    plt.xlabel('Date')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def run_cointegration_pipeline(uba_file_path, gtco_file_path):
    """
    Main pipeline: loads, processes, plots, and tests for cointegration on prices and returns.
    """
    uba = load_and_prepare_data(uba_file_path, 'FIDELITY')
    gtco = load_and_prepare_data(gtco_file_path, 'ZENITHBANK')

    df_combined = pd.merge(uba, gtco, left_index=True, right_index=True).dropna()

    # 1. Plot raw prices
    plot_series(df_combined, 'FIDELITY vs ZENITHBANK Closing Prices (2015–2025)')

    # 2. Log Prices
    df_log = np.log(df_combined)
    plot_series(df_log, 'FIDELITY vs ZENITHBANK Log Prices')
    perform_engle_granger_test(df_log['FIDELITY_close'], df_log['ZENITHBANK_close'], '(Log Prices)')
    perform_johansen_test(df_log, '(Log Prices)')

    # # 3. Log Returns
    # df_log_returns = df_log.diff().dropna()
    # plot_series(df_log_returns, 'UBA vs GTCO Log Returns')
    # perform_engle_granger_test(df_log_returns['UBA_close'], df_log_returns['GTCO_close'], '(Log Returns)')
    # perform_johansen_test(df_log_returns, '(Log Returns)')

def main():
    zenith_file = "/content/BLACKBULL_NAS100, 3_c071d.csv"
    uba_file = "/content/CAPITALCOM_US100, 3_05d08.csv"

    try:
        run_cointegration_pipeline(zenith_file, uba_file)
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

def load_data(filepath, ticker_name):
    """Load OHLCV data and return close prices with datetime index."""
    df = pd.read_csv(filepath, parse_dates=['time'])
    df = df[['time', 'close']].dropna()
    df = df.rename(columns={'close': f'{ticker_name}_close'})
    df.set_index('time', inplace=True)
    return df

def calculate_hedge_ratio(y, x):
    """Calculate hedge ratio using OLS regression."""
    x = sm.add_constant(x)  # Adds intercept
    model = sm.OLS(y, x).fit()
    hedge_ratio = model.params[1]  # slope is the hedge ratio
    intercept = model.params[0]
    return hedge_ratio, intercept

def calculate_spread(y, x, hedge_ratio, intercept=0):
    """Calculate spread using hedge ratio."""
    return y - (hedge_ratio * x + intercept)

def calculate_zscore(spread, window=20):
    """Calculate rolling Z-score of the spread."""
    spread_mean = spread.rolling(window=window).mean()
    spread_std = spread.rolling(window=window).std()
    z_score = (spread - spread_mean) / spread_std
    return z_score

# -------------------- Usage ---------------------------

# Define your file paths here
file1 = '/content/BLACKBULL_NAS100, 3_c071d.csv'  # Example: 'data/US100.csv'
file2 = '/content/CAPITALCOM_US100, 3_05d08.csv' # Example: 'data/NAS100.csv'

# Load data
equity1 = load_data(file1, "Equity1")
equity2 = load_data(file2, "Equity2")

# Combine and align datasets
df = pd.merge(equity1, equity2, left_index=True, right_index=True).dropna()

# Calculate hedge ratio
hedge_ratio, intercept = calculate_hedge_ratio(df['Equity1_close'], df['Equity2_close'])
print(f"Hedge Ratio: {hedge_ratio:.4f}, Intercept: {intercept:.4f}")

# Calculate spread
spread = calculate_spread(df['Equity1_close'], df['Equity2_close'], hedge_ratio, intercept)

# Calculate z-score
z_score = calculate_zscore(spread)

# Add results to dataframe
df['spread'] = spread
df['z_score'] = z_score

# Display head
print(df.head())

# Plot
plt.figure(figsize=(14, 6))
plt.plot(df.index, df['z_score'], label="Z-Score", color='purple')
plt.axhline(0, color='black', linestyle='--')
plt.axhline(2, color='red', linestyle='--', label='Upper Threshold')
plt.axhline(-2, color='green', linestyle='--', label='Lower Threshold')
plt.title("Z-Score of Spread")
plt.xlabel("Date")
plt.ylabel("Z-Score")
plt.legend()
plt.show()



import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt



# Load CSVs (ZENITH and UBA should be aligned by date)
zenith = pd.read_csv("/content/NSENG_ZENITHBANK, 1D_6d9c1.csv", parse_dates=["time"], index_col="time")
uba = pd.read_csv("/content/NSENG_UBA, 1D_d298c.csv", parse_dates=["time"], index_col="time")

# Extract log prices
zenith["log_price"] = np.log(zenith["close"])
uba["log_price"] = np.log(uba["close"])

# Merge the log prices and ensure aligned indices
data = pd.DataFrame({
    "log_ZENITH": zenith["log_price"],
    "log_UBA": uba["log_price"]
}).dropna()  # Drop rows with missing values to align indices

X = sm.add_constant(data["log_UBA"])
model = sm.OLS(data["log_ZENITH"], X).fit()
beta = model.params["log_UBA"]
alpha = model.params["const"]
print(f"β = {beta:.4f}, α = {alpha:.4f}")
data["spread"] = data["log_ZENITH"] - beta * data["log_UBA"]

# Static Z-score (based on entire history)
mean_spread = data["spread"].mean()
std_spread = data["spread"].std()
data["zscore"] = (data["spread"] - mean_spread) / std_spread

# Optional: Rolling Z-score (e.g., 60-day window)
# data["zscore_rolling"] = (data["spread"] - data["spread"].rolling(60).mean()) / data["spread"].rolling(60).std()

# Example: GTCO and UBA log prices
X = zenith["log_price"]
Y = uba["log_price"]

# Ensure X and Y have the same index before regression
# Reindex X and Y to the intersection of their indices
common_index = X.index.intersection(Y.index)
X = X.reindex(common_index)
Y = Y.reindex(common_index)


# Regression 1: GTCO ~ UBA
X1 = sm.add_constant(X)
model1 = sm.OLS(Y, X1).fit()
resid1 = model1.resid
std1 = np.std(resid1)

# Regression 2: UBA ~ GTCO
X2 = sm.add_constant(Y)
model2 = sm.OLS(X, X2).fit()
resid2 = model2.resid
std2 = np.std(resid2)

if std1 < std2:
    print("zenith is the pacesetter (independent variable)")
else:
    print("uba is the pacesetter (independent variable)")



plt.figure(figsize=(14, 6))
plt.plot(data.index, data["zscore"], label="Z-score")
plt.axhline(0, color="black")
plt.axhline(1.0, color="red", linestyle="--", label="Sell Signal")
plt.axhline(-1.0, color="green", linestyle="--", label="Buy Signal")
plt.title("Z-Score of Spread between ZENITHBANK and UBA")
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import statsmodels.api as sm
import os

def load_close_series(filepath, ticker_name):
    """
    Load CSV data and extract the closing price with date as index.
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")

    df = pd.read_csv(filepath, parse_dates=['time'])
    df = df[['time', 'close']].dropna()
    df.rename(columns={'close': f'{ticker_name}_close'}, inplace=True)
    df.set_index('time', inplace=True)
    df = df['2015-01-01':'2025-12-31']
    return df

def determine_pacesetter(file1_path, file2_path, ticker1_name, ticker2_name):
    """
    Determine the pacesetter stock by comparing residual standard deviations
    from two regressions: ticker1 ~ ticker2 and ticker2 ~ ticker1.
    """
    # Load data
    df1 = load_close_series(file1_path, ticker1_name)
    df2 = load_close_series(file2_path, ticker2_name)

    # Merge dataframes
    df = pd.merge(df1, df2, left_index=True, right_index=True).dropna()

    # Regression 1: ticker1 ~ ticker2
    X1 = sm.add_constant(df[f'{ticker2_name}_close'])
    model1 = sm.OLS(df[f'{ticker1_name}_close'], X1).fit()
    resid_std1 = np.std(model1.resid)

    # Regression 2: ticker2 ~ ticker1
    X2 = sm.add_constant(df[f'{ticker1_name}_close'])
    model2 = sm.OLS(df[f'{ticker2_name}_close'], X2).fit()
    resid_std2 = np.std(model2.resid)

    # Print results
    print("\n--- Residual Standard Deviations ---")
    print(f"{ticker1_name} ~ {ticker2_name}: {resid_std1:.4f}")
    print(f"{ticker2_name} ~ {ticker1_name}: {resid_std2:.4f}")

    # Determine pacesetter
    if resid_std1 < resid_std2:
        print(f"\n✅ {ticker2_name} is the pacesetter (lower residual std).")
        return ticker2_name
    else:
        print(f"\n✅ {ticker1_name} is the pacesetter (lower residual std).")
        return ticker1_name

# Example usage:
if __name__ == "__main__":
    pacesetter = determine_pacesetter(
        "/content/NSENG_FBNH, 1D_15c8b.csv",
        "/content/NSENG_ACCESSCORP, 1D_71fee.csv",
        "FBNH", "ACCESSBANK"
    )

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import zscore
import os

def load_close_series(filepath, ticker_name):
    df = pd.read_csv(filepath, parse_dates=['time'])
    df = df[['time', 'close']].dropna()
    df.rename(columns={'close': f'{ticker_name}_close'}, inplace=True)
    df.set_index('time', inplace=True)
    df = df['2015-01-01':'2025-12-31']
    return df

def run_pairs_trading_strategy(file1, file2, ticker1, ticker2):
    df1 = load_close_series(file1, ticker1)
    df2 = load_close_series(file2, ticker2)
    df = pd.merge(df1, df2, left_index=True, right_index=True).dropna()

    # Regress in both directions
    X1 = sm.add_constant(df[f'{ticker2}_close'])
    model1 = sm.OLS(df[f'{ticker1}_close'], X1).fit()
    resid_std1 = np.std(model1.resid)

    X2 = sm.add_constant(df[f'{ticker1}_close'])
    model2 = sm.OLS(df[f'{ticker2}_close'], X2).fit()
    resid_std2 = np.std(model2.resid)

    if resid_std1 < resid_std2:
        pacesetter = f'{ticker2}_close'
        dependent = f'{ticker1}_close'
        beta = model1.params[1]
    else:
        pacesetter = f'{ticker1}_close'
        dependent = f'{ticker2}_close'
        beta = model2.params[1]

    # Calculate spread & z-score
    spread = df[dependent] - beta * df[pacesetter]
    zscore_spread = zscore(spread)

    # Generate signals
    df['Spread'] = spread
    df['Z-Score'] = zscore_spread
    df['Signal'] = None
    df.loc[zscore_spread > 1, 'Signal'] = 'Sell Dependent / Buy Pacesetter'
    df.loc[zscore_spread < -1, 'Signal'] = 'Buy Dependent / Sell Pacesetter'
    df.loc[(zscore_spread >= -0.5) & (zscore_spread <= 0.5), 'Signal'] = 'Close Position'
    df['Position'] = 0
    df['Cooldown'] = (df['Signal'].notnull()).astype(int).rolling(window=5).sum() > 0
    df['Position'] = np.where(df['Cooldown'], 0, df['Position'])
    df.loc[zscore_spread > 1, 'Position'] = -1
    df.loc[zscore_spread < 1, 'Position'] = 1

    # Backtest
    df['Position_shifted'] = df['Position'].shift(1).fillna(0)
    spread_return = df[dependent].pct_change() - beta * df[pacesetter].pct_change()
    df['Spread Return'] = spread_return
    df['Strategy Return'] = df['Position_shifted'] * df['Spread Return']
    df['Cumulative Return'] = (1 + df['Strategy Return']).cumprod()

    # Plot cumulative return
    plt.figure(figsize=(14, 6))
    plt.plot(df['Cumulative Return'], label='Cumulative Strategy Return')
    plt.title('Cumulative Return')
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Plot entry/exit signals
    entry = df[df['Signal'].isin(['Buy Dependent / Sell Pacesetter', 'Sell Dependent / Buy Pacesetter'])]
    exit_ = df[df['Signal'] == 'Close Position']

    plt.figure(figsize=(14, 6))
    plt.plot(df[dependent], label='Dependent', alpha=0.6)
    plt.plot(df[pacesetter], label='Pacesetter', alpha=0.6)
    plt.scatter(entry.index, df.loc[entry.index, dependent], color='green', label='Entry', marker='^')
    plt.scatter(exit_.index, df.loc[exit_.index, dependent], color='red', label='Exit', marker='v')
    plt.title('Entry & Exit Points')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Export
    output_path = "/content/pairs_trading_output.csv"
    df.to_csv(output_path)
    return output_path

# Run
run_pairs_trading_strategy(
    "/NSENG_FBNH, 1D_15c8b.csv",
    "/NSENG_ACCESSCORP, 1D_71fee.csv",
    "FBNH",
    "ACCESSBANK"
)